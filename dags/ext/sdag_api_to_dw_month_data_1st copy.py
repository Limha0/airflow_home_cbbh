import logging
import os
import urllib3
import sys

from pendulum import datetime, from_format,now
from airflow.decorators import dag, task, task_group
from airflow.providers.postgres.hooks.postgres import PostgresHook
from sqlalchemy.orm import sessionmaker
from dto.th_data_clct_mastr_log import ThDataClctMastrLog
from dto.tn_clct_file_info import TnClctFileInfo
from util.file_util import FileUtil
from util.common_util import CommonUtil
from dto.tc_com_dtl_cd import TcCmmnDtlCd as CONST

from util.date_custom_util import DateUtil
from dto.tn_data_bsc_info import TnDataBscInfo
from dto.tdm_list_url_info import TdmListUrlInfo
from dto.tdm_file_url_info import TdmFileUrlInfo
from dto.tdm_standard_url_info import TdmStandardUrlInfo
from airflow.exceptions import AirflowSkipException

"""
ì¶”ê°€ API í˜¸ì¶œí•˜ì—¬ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° CSV ì €ì¥
"""
import re
import json
import pandas as pd
import os
from io import BytesIO
from xml_to_dict import XMLtoDict
import requests
from requests.exceptions import RequestException
import time


def safe_get(url, max_retries=5, backoff_factor=2, timeout=20):
    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(url, verify=False, timeout=timeout)
            response.raise_for_status()
            return response
        except RequestException as e:
            logging.warning(f"ğŸ” ìš”ì²­ ì‹¤íŒ¨ {attempt}/{max_retries}íšŒ: {e}")
            if attempt < max_retries:
                time.sleep(backoff_factor ** attempt)
            else:
                raise

def call_additional_api_and_save_csv(row, output_dir, null_atchfile_count, valid_atchfile_count, api_error_count):

    list_id = row.get('list_id')
    id_ = row.get('id')
    title = row.get('title') or f"{list_id}_{id_}"

    if not list_id or not id_:
        logging.warning(f"âš ï¸ list_id ë˜ëŠ” id ì—†ìŒ â†’ ìƒëµë¨: {row}")
        return null_atchfile_count, valid_atchfile_count, api_error_count

    new_url = f"https://www.data.go.kr/tcs/dss/selectFileDataDownload.do?recommendDataYn=Y&publicDataPk={list_id}&publicDataDetailPk={id_}"
    try:
        try:
            # ì¼ì‹œì  api í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
            resp = safe_get(new_url)
            data = resp.json()
        except Exception as e:
            api_error_count[0] += 1
            logging.warning(f"âŒ ì¶”ê°€ API í˜¸ì¶œ ì‹¤íŒ¨: {e}, URL: {new_url}")
            return null_atchfile_count, valid_atchfile_count, api_error_count

        file_info = data.get('fileDataRegistVO') or {}
        atchFileId = file_info.get('atchFileId')
        fileDetailSn = file_info.get('fileDetailSn')
        dataNm = file_info.get('dataNm')
        orginlFileNm = file_info.get('orginlFileNm') or ''

        if not all([atchFileId, fileDetailSn, dataNm]):
            null_atchfile_count[0] += 1
            logging.warning(f"âš ï¸ ë©”íƒ€ì •ë³´ ëˆ„ë½: {title}, list_id: {list_id}, id: {id_}")
            return null_atchfile_count, valid_atchfile_count, api_error_count

        safe_dataNm = re.sub(r'[^\w\-.]', '_', str(dataNm))
        _, ext = os.path.splitext(orginlFileNm.lower())
        raw_path = os.path.join(output_dir, f"{safe_dataNm}{ext}")
        csv_path = os.path.join(output_dir, f"{safe_dataNm}.csv")

        download_url = f"https://www.data.go.kr/cmm/cmm/fileDownload.do?atchFileId={atchFileId}&fileDetailSn={fileDetailSn}&dataNm={dataNm}"

        try:
            download_resp = safe_get(download_url)
        except Exception as e:
            null_atchfile_count[0] += 1
            logging.warning(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ í›„ ì‹¤íŒ¨: {e}, URL: {download_url}")
            return null_atchfile_count, valid_atchfile_count, api_error_count

        with open(raw_path, 'wb') as f:
            f.write(download_resp.content)
        logging.info(f"âœ… ì›ë³¸ ì €ì¥ ì™„ë£Œ: {raw_path}")
        valid_atchfile_count[0] += 1

        # CSV ì €ì¥ ë¡œì§
        if ext == ".csv":
            try:
                # ì¸ì½”ë”© ê°ì§€ ë° ë””ì½”ë”© ì‹œë„
                try:
                    decoded_text = download_resp.content.decode("utf-8-sig")
                except UnicodeDecodeError:
                    decoded_text = download_resp.content.decode("cp949")

                with open(csv_path, 'w', encoding='utf-8-sig') as f:
                    f.write(decoded_text)
                logging.info(f"âœ… CSV í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {csv_path}")
            except Exception as e:
                null_atchfile_count[0] += 1
                logging.warning(f"âŒ CSV í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

        elif ext == ".xml":
            try:
                xml_dict = XMLtoDict().parse(download_resp.text)
                root_key = next(iter(xml_dict))
                # xml_dict = {'root': {'header': {...}, 'Row': [...]}},  root_key = 'root'
                content = xml_dict[root_key]

                # rowê°€ í•˜ë‚˜ì¼ë•Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ csvë¡œ ì €ì¥
                # Row ë…¸ë“œê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ë‚˜ ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ
                row_data = None
                if "Row" in content:
                    row_data = content["Row"]
                else:
                    row_data = next((v for k, v in content.items() if isinstance(v, list) or isinstance(v, dict)), None)

                # ë‹¨ì¼ dictì¼ ê²½ìš° listë¡œ ê°ì‹¸ê¸°
                if isinstance(row_data, dict):
                    row_data = [row_data]
                elif not isinstance(row_data, list):
                    row_data = []
                    

                df = pd.json_normalize(row_data)
                if df.empty:
                    logging.warning("âš  XML íŒŒì‹± ê²°ê³¼ ë¹„ì–´ ìˆìŒ")
                else:
                    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                    logging.info(f"âœ… XML âœ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")
            except Exception as e:
                null_atchfile_count[0] += 1
                logging.warning(f"âŒ XML âœ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")

        elif ext in [".xlsx", ".xls"]:
            try:
                # XLSX íŒŒì¼ì€ ì‹œê° ìš”ì†Œê°€ í¬í•¨ëœ ë¬¸ì„œë¡œ íŒë‹¨ë˜ì–´
                # CSV ë³€í™˜ ìƒëµí•˜ê³  ì›ë³¸ë§Œ ìœ ì§€
                logging.info(f"âœ… XLSX íŒŒì¼ì€ ì‹œê° ìš”ì†Œê°€ í¬í•¨ëœ ë¬¸ì„œë¡œ íŒë‹¨ë˜ì–´ ë³€í™˜ ìƒëµ: {raw_path}")
            except Exception as e:
                null_atchfile_count[0] += 1
                logging.warning(f"âŒ XLSX âœ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")

            # XLSX íŒŒì¼ì„ pandasë¡œ ì½ì–´ CSVë¡œ ë³€í™˜
            # pandasë¡œ ì½ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ìˆì–´ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìš” => CSV ë³€í™˜ ìƒëµí•˜ê³  ì›ë³¸ë§Œ ìœ ì§€
            # ====> ì‹œê°í™” ìœ„ì£¼ì¸ë°ë„ csv ë³€í™˜ì„ ì‹œë„í•˜ëŠ” ê²½ìš°ê°€ ìˆì–´ ì£¼ì„ ì²˜ë¦¬ 20250731
        
            # try:
            #     from io import BytesIO
            #     import pandas as pd

            #     excel_file = BytesIO(download_resp.content)
            #     xls = pd.ExcelFile(excel_file)
            #     sheet = xls.sheet_names[0]
            #     df = xls.parse(sheet)

            #     # ì •í˜• ë°ì´í„°ê°€ ì‹¤ì œ ì¡´ì¬í•  ê²½ìš°ë§Œ CSV ë³€í™˜
            #     if df.empty or df.columns.size == 0:
            #         logging.info(f"âš  ì‹œê°í™” ìœ„ì£¼ì˜ XLSXë¡œ íŒë‹¨ë˜ì–´ CSV ë³€í™˜ ìƒëµ: {raw_path}")
            #     else:
            #         df.to_csv(csv_path, index=False, encoding="utf-8-sig")
            #         logging.info(f"âœ… XLSX âœ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

            # except Exception as e:
            #     null_atchfile_count[0] += 1
            #     logging.warning(f"âŒ XLSX âœ CSV ë³€í™˜ ì‹¤íŒ¨: {e}")

        else:
            logging.warning(f"âš  ì§€ì›ë˜ì§€ ì•ŠëŠ” í™•ì¥ì: {ext} - ì›ë³¸ë§Œ ì €ì¥ë¨")

    except json.JSONDecodeError:
        null_atchfile_count[0] += 1
        logging.warning(f"âŒ ë©”íƒ€ì •ë³´ JSON íŒŒì‹± ì‹¤íŒ¨: {new_url}")
    except Exception as e:
        api_error_count[0] += 1
        logging.warning(f"âŒ ì¶”ê°€ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    return null_atchfile_count, valid_atchfile_count, api_error_count

@dag(
    dag_id="sdag_api_dw_month_data_1st",
    schedule="@monthly",
    start_date=datetime(2025, 7, 28, tz="Asia/Seoul"),  # UI ì— KST ì‹œê°„ìœ¼ë¡œ í‘œì¶œí•˜ê¸° ìœ„í•œ tz ì„¤ì •
    catchup=False,
    # render Jinja template as native Python object
    render_template_as_native_obj=True,
    tags=["api_to_csv", "month", "ext","data"],
)
def api_dw_month_data_1st():
    # PostgresHook ê°ì²´ ìƒì„±
    pg_hook = PostgresHook(postgres_conn_id='gsdpdb_db_conn')

    #sqlalchemy ë¥¼ ì´ìš©í•œ connection
    engine = pg_hook.get_sqlalchemy_engine()

    # sqlalchey session ìƒì„±
    session = sessionmaker(engine, expire_on_commit=False)

    @task
    def collect_data_info(**kwargs): # ìˆ˜ì§‘ ë°ì´í„° ì •ë³´ ì¡°íšŒ
        """
        tn_data_bsc_infoí…Œì´ë¸”ì—ì„œ ìˆ˜ì§‘ ëŒ€ìƒ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ í›„ th_data_clct_mastr_log í…Œì´ë¸”ì— ì…ë ¥
        return: collect_data_list
        """
        print("hello")
        # print("kwargs :" , kwargs)
        select_bsc_info_stmt = '''
                                select *, (SELECT dtl_cd_nm FROM tc_com_dtl_cd WHERE group_cd = 'pvdr_site_cd' AND pvdr_site_cd = dtl_cd) AS pvdr_site_nm
                                from tn_data_bsc_info
                                where
                                    use_yn = 'y'
                                and data_rls_se_cd = 'un_othbc'
                                and pvdr_site_cd = 'ps00005'
                                and dtst_cd not in (
                                'data919'
                                ,'data920'
                                ,'data922'    
                                )
                                --and dtst_cd in ('data10010','data10022','data10034')--ì œì²œì‹œ ë§ê³  ì´ì œ ì „ì²´ë¡œ ë‹¤ ëŒë ¤ë³´ê¸°
                                order by sn
                            '''
        data_interval_start = kwargs['data_interval_start'].in_timezone("Asia/Seoul")  # ì²˜ë¦¬ ë°ì´í„°ì˜ ì‹œì‘ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ ì‹œì )
        data_interval_end = kwargs['data_interval_end'].in_timezone("Asia/Seoul")   # ì‹¤ì œ ì‹¤í–‰í•˜ëŠ” ë‚ ì§œë¥¼ KST ë¡œ ì„¤ì •
        # collect_data_list = []
        collect_data_list = CommonUtil.insert_collect_data_info(select_bsc_info_stmt, session, data_interval_start, data_interval_end, kwargs)
        collect_data_list = []
        try:
            with session.begin() as conn:
                for dict_row in conn.execute(select_bsc_info_stmt).all():
                    tn_data_bsc_info = TnDataBscInfo(**dict_row)

                    data_crtr_pnttm = CommonUtil.set_data_crtr_pnttm(tn_data_bsc_info.link_clct_cycle_cd, data_interval_start)
                    file_name = tn_data_bsc_info.dtst_nm.replace(" ", "_") + "20250728"

                    # th_data_clct_mastr_log set
                    th_data_clct_mastr_log = ThDataClctMastrLog()
                    th_data_clct_mastr_log.dtst_cd = tn_data_bsc_info.dtst_cd
                    th_data_clct_mastr_log.dtst_dtl_cd = tn_data_bsc_info.dtst_dtl_cd
                    th_data_clct_mastr_log.clct_ymd = data_interval_end.strftime("%Y%m%d")
                    th_data_clct_mastr_log.clct_data_nm = tn_data_bsc_info.dtst_nm
                    th_data_clct_mastr_log.data_crtr_pnttm = data_crtr_pnttm
                    th_data_clct_mastr_log.reclect_flfmt_nmtm = 0
                    th_data_clct_mastr_log.step_se_cd = CONST.STEP_CNTN
                    th_data_clct_mastr_log.stts_cd = CONST.STTS_WORK
                    th_data_clct_mastr_log.stts_dt = now(tz="UTC")
                    th_data_clct_mastr_log.stts_msg = CONST.MSG_CNTN_WORK
                    th_data_clct_mastr_log.crt_dt = now(tz="UTC")

                    # tn_clct_file_info ìˆ˜ì§‘íŒŒì¼ì •ë³´ set
                    tn_clct_file_info = CommonUtil.set_file_info(TnClctFileInfo(), th_data_clct_mastr_log, file_name, None, tn_data_bsc_info.link_file_extn, None, None)

                    collect_data_list.append({
                                            "tn_data_bsc_info" : tn_data_bsc_info.as_dict()
                                            , "th_data_clct_mastr_log": th_data_clct_mastr_log.as_dict()
                                            , "tn_clct_file_info": tn_clct_file_info.as_dict()
                                            })
        except Exception as e:
            logging.info(f"insert_collect_data_info Exception::: {e}")
            raise e
        return collect_data_list


    @task_group(group_id='call_url_process')
    def call_url_process(collect_data_list):

        @task
        def create_directory(collect_data_list, **kwargs):
            """
            ìˆ˜ì§‘ íŒŒì¼ ê²½ë¡œ ìƒì„±
            params: tn_data_bsc_info, th_data_clct_mastr_log, tn_clct_file_info
            return: file_path: tn_clct_file_info í…Œì´ë¸”ì— ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            """
            data_interval_end = kwargs['data_interval_end'].in_timezone("Asia/Seoul")  # ì‹¤ì œ ì‹¤í–‰í•˜ëŠ” ë‚ ì§œë¥¼ KST ë¡œ ì„¤ì •
            root_collect_file_path = kwargs['var']['value'].root_collect_file_path
            temp_list = []
            if isinstance(collect_data_list, list):  # list ì¸ ê²½ìš°
                temp_list.extend(collect_data_list)
            else:  # dict ì¸ ê²½ìš°
                temp_list.append(collect_data_list)
            for collect_data_dict in temp_list:
                tn_data_bsc_info = TnDataBscInfo(**collect_data_dict['tn_data_bsc_info'])

                # íŒŒì¼ ê²½ë¡œ ì„¤ì •
                file_path, full_file_path = CommonUtil.set_file_path(root_collect_file_path, data_interval_end, tn_data_bsc_info)
            try:
                # ìˆ˜ì§‘ í´ë” ê²½ë¡œ ìƒì„±
                os.makedirs(full_file_path, exist_ok=True)
            except OSError as e:
                logging.info(f"create_directory OSError::: {e}")
                raise AirflowSkipException()
            logging.info(f"create_directory full_file_path::: {full_file_path}")
            return file_path
        
        @task
        def call_url(collect_data_list,file_path,**kwargs):
            """
            ì¡°ê±´ë³„ URL ì„¤ì • ë° í˜¸ì¶œí•˜ì—¬ dw ì ì¬
            params: tdm_list_url_info, tdm_file_url_info, tdm_standard_url_info, th_data_clct_mastr_log, tn_clct_file_info, file_path
            return: file_size
            """
            import requests
            import os
            import time
            from util.call_url_util import CallUrlUtil
            from xml_to_dict import XMLtoDict
            from sqlalchemy import text

            tn_data_bsc_info = TnDataBscInfo(**collect_data_list['tn_data_bsc_info'])
            th_data_clct_mastr_log = ThDataClctMastrLog(**collect_data_list['th_data_clct_mastr_log'])
            tn_clct_file_info = TnClctFileInfo(**collect_data_list['tn_clct_file_info'])
            # log_full_file_path = collect_data_list['log_full_file_path']
            root_collect_file_path = kwargs['var']['value'].root_collect_file_path

            dtst_cd = th_data_clct_mastr_log.dtst_cd.lower()
            link_se_cd = tn_data_bsc_info.link_se_cd.lower()
            root_collect_file_path = kwargs['var']['value'].root_collect_file_path
            pvdr_site_cd = tn_data_bsc_info.pvdr_site_cd.lower()
            pvdr_inst_cd = tn_data_bsc_info.pvdr_inst_cd.lower()
            base_url = return_url = tn_data_bsc_info.link_data_clct_url

            # íŒŒë¼ë¯¸í„° ë° íŒŒë¼ë¯¸í„° ê¸¸ì´ ì„¤ì •
            data_interval_start = now()  # ì²˜ë¦¬ ë°ì´í„°ì˜ ì‹œì‘ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ ì‹œì )
            data_interval_end = now()  # ì‹¤ì œ ì‹¤í–‰í•˜ëŠ” ë‚ ì§œë¥¼ KST ë¡œ ì„¤ì •
            params_dict, params_len = CallUrlUtil.set_params(tn_data_bsc_info, session, data_interval_start, data_interval_end, kwargs)

            retry_num = 0  # ë°ì´í„° ì—†ì„ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
            repeat_num = 1  # íŒŒë¼ë¯¸í„° ê¸¸ì´ë§Œí¼ ë°˜ë³µ í˜¸ì¶œ íšŸìˆ˜
            page_no = 1  # í˜„ì¬ í˜ì´ì§€
            total_page = 1  # ì´ í˜ì´ì§€ ìˆ˜
            
            header = True   # íŒŒì¼ í—¤ë” ëª¨ë“œ
            mode = "w"  # íŒŒì¼ ì“°ê¸° ëª¨ë“œ overwrite

             # ë°ì´í„°ì…‹ ì½”ë“œë³„ íŒŒì¼ ì´ë¦„
            if dtst_cd in ('data10012'
                           ,'data10013'
                           ,'data10014'
                           ,'data10015'
                           ,'data10016'
                           ,'data10017'
                           ,'data10018'
                           ,'data10019'
                           ,'data10020'
                           ,'data10021'
                           ,'data10022'
                           ,'data10023' ):
                table_name = TdmListUrlInfo.__tablename__
            # 
            elif dtst_cd in ('data10000'
                            ,'data10001'
                            ,'data10002'
                            ,'data10003'
                            ,'data10004'
                            ,'data10005'
                            ,'data10006'
                            ,'data10007'
                            ,'data10008'
                            ,'data10009'
                            ,'data10010'
                            ,'data10011'):
                table_name = TdmFileUrlInfo.__tablename__
            else :
                table_name = TdmStandardUrlInfo.__tablename__

            link_file_crt_yn = tn_data_bsc_info.link_file_crt_yn.lower()  # csv íŒŒì¼ ìƒì„± ì—¬ë¶€
            file_name = tn_clct_file_info.insd_file_nm + "." + tn_clct_file_info.insd_file_extn  # csv íŒŒì¼ëª…
            source_file_name =  tn_clct_file_info.insd_file_nm + "." + tn_data_bsc_info.pvdr_sou_data_pvsn_stle  # ì›ì²œ íŒŒì¼ëª…
            full_file_path = root_collect_file_path + file_path
            full_file_name = full_file_path + file_name
            link_file_sprtr = tn_data_bsc_info.link_file_sprtr
            file_size = 0  # íŒŒì¼ ì‚¬ì´ì¦ˆ
            row_count = 0  # í–‰ ê°œìˆ˜

            # ì¹´ìš´í„° ì´ˆê¸°í™” (file-data-list íƒ€ì…ì—ì„œë§Œ ì‚¬ìš©)
            null_atchfile_count = [0]
            valid_atchfile_count = [0]
            api_error_count = [0]

            try:
                # íŒŒë¼ë¯¸í„° ê¸¸ì´ë§Œí¼ ë°˜ë³µ í˜¸ì¶œ
                while repeat_num <= params_len:
                    
                    # ì´ í˜ì´ì§€ ìˆ˜ë§Œí¼ ë°˜ë³µ í˜¸ì¶œ
                    while page_no <= total_page:
                        
                        # íŒŒë¼ë¯¸í„° ê¸¸ì´ë§Œí¼ í˜¸ì¶œ ì‹œ while ì¢…ë£Œ
                        if repeat_num > params_len:
                            break
                    
                        # ì¬ì‹œë„ 5íšŒ ì´ìƒ ì‹œ
                        if retry_num >= 5:
                            # íŒŒë¼ë¯¸í„° ê¸¸ì´ == 1) whlie ì¢…ë£Œ
                            if params_len == 1:
                                repeat_num += 1
                                break
                            else:  # íŒŒë¼ë¯¸í„° ê¸¸ì´ != 1)
                                # th_data_clct_contact_fail_hstry_log ì— ì…ë ¥
                                CallUrlUtil.insert_fail_history_log(th_data_clct_mastr_log, return_url, file_path, session, params_dict['param_list'][repeat_num - 1], page_no)
                                # ì´ í˜ì´ì§€ ìˆ˜ë§Œí¼ ëœ ëŒì•˜ì„ ë•Œ
                                if page_no < total_page:  # ë‹¤ìŒ í˜ì´ì§€ í˜¸ì¶œ
                                    retry_num = 0
                                    page_no += 1
                                    continue
                                # ì´ í˜ì´ì§€ ìˆ˜ë§Œí¼ ë‹¤ ëŒê³ 
                                elif page_no == total_page:
                                    # íŒŒë¼ë¯¸í„° ê¸¸ì´ë§Œí¼ ëœ ëŒì•˜ì„ ë•Œ
                                    if repeat_num < params_len:
                                        retry_num = 0
                                        page_no = 1
                                        repeat_num += 1
                                        continue
                                    # íŒŒë¼ë¯¸í„° ê¸¸ì´ë§Œí¼ ë‹¤ ëŒì•˜ì„ ë•Œ
                                    else:
                                        repeat_num += 1
                                        break
                        # url ì„¤ì •
                        return_url = f"{base_url}{CallUrlUtil.set_url(dtst_cd, link_se_cd, pvdr_site_cd, pvdr_inst_cd, params_dict, repeat_num, page_no)}"

                        # url í˜¸ì¶œ
                        response = requests.get(return_url, verify= False)
                        response_code = response.status_code        

                        # url í˜¸ì¶œ ì‹œ ë©”ì„¸ì§€ ì„¤ì •
                        header, mode = CallUrlUtil.get_request_message(retry_num, repeat_num, page_no, return_url, total_page, full_file_name, header, mode)

                        if response_code == 200:
                            if tn_data_bsc_info.pvdr_sou_data_pvsn_stle == "json" and 'OpenAPI_ServiceResponse' not in response.text:  # ê³µê³µë°ì´í„°í¬í„¸ - HTTP ì—ëŸ¬ ì œì™¸
                                json_data = response.json()
                            if tn_data_bsc_info.pvdr_sou_data_pvsn_stle == "xml" or 'OpenAPI_ServiceResponse' in response.text:  # ê³µê³µë°ì´í„°í¬í„¸ - HTTP ì—ëŸ¬ ì‹œ xml í˜•íƒœ
                                json_data = XMLtoDict().parse(response.text)

                            # ì›ì²œ ë°ì´í„° ì €ì¥
                            CallUrlUtil.create_source_file(json_data, source_file_name, full_file_path, mode)

                            # ê³µê³µë°ì´í„°í¬í„¸ - HTTP ì—ëŸ¬ ì‹œ
                            if 'OpenAPI_ServiceResponse' in response.text:
                                retry_num += 1
                                continue

                            result = CallUrlUtil.read_json(json_data, pvdr_site_cd, pvdr_inst_cd, dtst_cd, tn_data_bsc_info.data_se_col_one)
                            result_json = result['result_json_array']
                            result_size = len(result_json)

                            # ë°ì´í„° êµ¬ë¶„ê°’ í™•ì¸
                            data_se_val_two = tn_data_bsc_info.pvdr_data_se_vl_two.lower() if tn_data_bsc_info.pvdr_data_se_vl_two else ''

                            # file-data-list íƒ€ì…ì¸ ê²½ìš° ì¶”ê°€ API í˜¸ì¶œ ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                            # if data_se_val_two == "file-data-list" or data_se_val_two == "standard-data-list":
                            if data_se_val_two in ("file-data-list", "standard-data-list"):

                                # âœ… ì¤‘ë³µ ì œê±°: list_id, id ê¸°ì¤€
                                unique_result_json = list({(item['list_id'], item['id']): item for item in result_json}.values())

                                # ê° í•­ëª©ì— ëŒ€í•´ ì¶”ê°€ API í˜¸ì¶œ
                                for dict_value in unique_result_json:
                                    null_atchfile_count, valid_atchfile_count, api_error_count = call_additional_api_and_save_csv(
                                        dict_value, full_file_path, null_atchfile_count, valid_atchfile_count, api_error_count
                                    )
                                
                                logging.info(f"ğŸ“Š í˜ì´ì§€ {page_no} ì¶”ê°€ íŒŒì¼ ìƒì„± í†µê³„ :: "
                                            f"API ì‘ë‹µ ê±´ìˆ˜: {len(result_json)}, "
                                            f"ì‹¤ì œ ì²˜ë¦¬ ì‹œë„: {valid_atchfile_count[0] + null_atchfile_count[0] + api_error_count[0]}, "
                                            f"ì •ìƒ ìƒì„±: {valid_atchfile_count[0]}, "
                                            f"ìƒëµ: {null_atchfile_count[0]}, "
                                            f"í˜¸ì¶œì‹¤íŒ¨: {api_error_count[0]}")
                                # logging.info(f"ğŸ“Š í˜ì´ì§€ {page_no} ì¶”ê°€ íŒŒì¼ ìƒì„± í†µê³„ :: ì´ ëŒ€ìƒ ê±´ìˆ˜: {valid_atchfile_count[0] + null_atchfile_count[0]+ api_error_count[0]}, "
                                #             f"ì •ìƒ ìƒì„±: {valid_atchfile_count[0]}, ìƒëµ: {null_atchfile_count[0]}, í˜¸ì¶œì‹¤íŒ¨: {api_error_count[0]}")

                            # ì¼ë°˜ì ì¸ CSV ì €ì¥ ë¡œì§ (ëª¨ë“  íƒ€ì…ì— ëŒ€í•´ ì‹¤í–‰)
                            if result_size != 0:
                                retry_num = 0  # ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê¸°í™”
                                if page_no == 1: # ì²« í˜ì´ì§€ì¼ ë•Œ
                                    # í˜ì´ì§• ê³„ì‚°
                                    total_count = int(result['total_count'])
                                    total_page = CallUrlUtil.get_total_page(total_count, result_size)

                                row_count = FileUtil.check_csv_length(link_file_sprtr, full_file_name)  # í–‰ ê°œìˆ˜ í™•ì¸
                                if row_count == 0:
                                    header = True
                                    mode = "w"

                                # csv íŒŒì¼ ìƒì„±
                                CallUrlUtil.create_csv_file(link_file_sprtr, th_data_clct_mastr_log.data_crtr_pnttm, th_data_clct_mastr_log.clct_log_sn, full_file_path, file_name, result_json, header, mode, page_no)

                                row_count = FileUtil.check_csv_length(link_file_sprtr, full_file_name)  # í–‰ ê°œìˆ˜ í™•ì¸
                                if row_count != 0:
                                    logging.info(f"í˜„ì¬ê¹Œì§€ íŒŒì¼ ë‚´ í–‰ ê°œìˆ˜: {row_count}")

                                    # DW ì ì¬ì‹œ clct_sn ì¦ê°€ê°’
                                    clct_sn_counter = 1
                                    
                                    # ë°ì´í„°ì…‹ ì½”ë“œë³„ ë°ì´í„° ì¶”ì¶œ
                                    extracted_data = []
                                    # ëª©ë¡ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì¸ ê²½ìš°
                                    for item in json_data.get('data', []): 
                                        if dtst_cd in ('data10012'
                                                      ,'data10013'
                                                      ,'data10014'
                                                      ,'data10015'
                                                      ,'data10016'
                                                      ,'data10017'
                                                      ,'data10018'
                                                      ,'data10019'
                                                      ,'data10020'
                                                      ,'data10021'
                                                      ,'data10022'
                                                      ,'data10023' ):
                                            extracted_data.append({
                                                'clct_sn': clct_sn_counter , 
                                                'category_cd': item.get('category_cd'), 
                                                'category_nm': item.get('category_nm'), 
                                                'collection_method': item.get('collection_method'), 
                                                'created_at': item.get('created_at'), 
                                                # '"desc"': item.get('desc'), 
                                                '"desc"': item.get('desc', '').replace('%', '%%') if item.get('desc') else None,
                                                'download_cnt': item.get('download_cnt'), 
                                                'ext': item.get('ext'), 
                                                'id': item.get('id'), 
                                                'is_deleted': item.get('is_deleted'), 
                                                'is_requested_data': item.get('is_requested_data'), 
                                                'keywords': item.get('keywords'), 
                                                'list_type': item.get('list_type'), 
                                                'new_category_cd': item.get('new_category_cd'), 
                                                'new_category_nm': item.get('new_category_nm'), 
                                                'org_cd': item.get('org_cd'), 
                                                'org_nm': item.get('org_nm'), 
                                                'ownership_grounds': item.get('ownership_grounds'), 
                                                'page_url': item.get('page_url'), 
                                                'providing_scope': item.get('providing_scope'), 
                                                'register_status': item.get('register_status'), 
                                                'title': item.get('title'), 
                                                'updated_at': item.get('updated_at'), 
                                                'view_cnt': item.get('view_cnt'), 
                                                'data_crtr_pnttm': th_data_clct_mastr_log.data_crtr_pnttm,
                                                'clct_pnttm': DateUtil.get_ymdhm(),
                                                'clct_log_sn': th_data_clct_mastr_log.clct_log_sn,
                                                'page_no': page_no
                                                # í•„ìš”í•œ ëª¨ë“  í•„ë“œë¥¼ ì¶”ê°€
                                            })
                                            clct_sn_counter += 1
                                        # íŒŒì¼ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì¸ ê²½ìš° 
                                        elif dtst_cd in ('data10000'
                                                        ,'data10001'
                                                        ,'data10002'
                                                        ,'data10003'
                                                        ,'data10004'
                                                        ,'data10005'
                                                        ,'data10006'
                                                        ,'data10007'
                                                        ,'data10008'
                                                        ,'data10009'
                                                        ,'data10010'
                                                        ,'data10011'):
                                            extracted_data.append({
                                                'clct_sn': clct_sn_counter , 
                                                'core_data_nm': item.get('core_data_nm'),
                                                'cost_unit': item.get('cost_unit'),
                                                'created_at': item.get('created_at'),
                                                'data_limit': item.get('data_limit'),
                                                'data_type': item.get('data_type'),
                                                'dept_nm': item.get('dept_nm'),
                                                # '"desc"': item.get('desc'),
                                                '"desc"': item.get('desc', '').replace('%', '%%') if item.get('desc') else None,
                                                'download_cnt': item.get('download_cnt'),
                                                'etc': item.get('etc'),
                                                'ext': item.get('ext'),
                                                'id': item.get('id'),
                                                'is_charged': item.get('is_charged'),
                                                'is_copyrighted': item.get('is_copyrighted'),
                                                'is_core_data': item.get('is_core_data'),
                                                'is_deleted': item.get('is_deleted'),
                                                'is_list_deleted': item.get('is_list_deleted'),
                                                'is_std_data': item.get('is_std_data'),
                                                'is_third_party_copyrighted': item.get('is_third_party_copyrighted'),
                                                'keywords': item.get('keywords'),
                                                'list_id': item.get('list_id'),
                                                'list_title': item.get('list_title'),
                                                'media_cnt': item.get('media_cnt'),
                                                'media_type': item.get('media_type'),
                                                'meta_url': item.get('meta_url'),
                                                'new_category_cd': item.get('new_category_cd'),
                                                'new_category_nm': item.get('new_category_nm'),
                                                'next_registration_date': item.get('next_registration_date'),
                                                'org_cd': item.get('org_cd'),
                                                'org_nm': item.get('org_nm'),
                                                'ownership_grounds': item.get('ownership_grounds'),
                                                'regist_type': item.get('regist_type'),
                                                'register_status': item.get('register_status'),
                                                'share_scope_nm': item.get('share_scope_nm'),
                                                'title': item.get('title'),
                                                'update_cycle': item.get('update_cycle'),
                                                'updated_at': item.get('updated_at'),
                                                'data_crtr_pnttm': th_data_clct_mastr_log.data_crtr_pnttm,
                                                'clct_pnttm': DateUtil.get_ymdhm(),
                                                'clct_log_sn': th_data_clct_mastr_log.clct_log_sn,
                                                'page_no': page_no
                                                # í•„ìš”í•œ ëª¨ë“  í•„ë“œë¥¼ ì¶”ê°€
                                            })
                                            clct_sn_counter += 1
                                        else:
                                            extracted_data.append({
                                                'clct_sn': clct_sn_counter ,
                                                'category_cd': item.get('category_cd'),
                                                'category_nm': item.get('category_nm'),
                                                'collection_method': item.get('collection_method'),
                                                'created_at': item.get('created_at'),
                                                'dept_nm': item.get('dept_nm'),
                                                # '"desc"': item.get('desc'),
                                                '"desc"': item.get('desc', '').replace('%', '%%') if item.get('desc') else None,
                                                'id': item.get('id'),
                                                'is_requested_data': item.get('is_requested_data'),
                                                'keywords': item.get('keywords'),
                                                'list_id': item.get('list_id'),
                                                'list_title': item.get('list_title'),
                                                'list_type': item.get('list_type'),
                                                'new_category_cd': item.get('new_category_cd'),
                                                'new_category_nm': item.get('new_category_nm'),
                                                'next_registration_date': item.get('next_registration_date'),
                                                'org_cd': item.get('org_cd'),
                                                'org_nm': item.get('org_nm'),
                                                'ownership_grounds': item.get('ownership_grounds'),
                                                'providing_scope': item.get('providing_scope'),
                                                'req_cnt': item.get('req_cnt'),
                                                'title': item.get('title'),
                                                'updated_dt': item.get('updated_dt'),
                                                'updated_dt': item.get('updated_dt'),
                                                'data_crtr_pnttm': th_data_clct_mastr_log.data_crtr_pnttm,
                                                'clct_pnttm': DateUtil.get_ymdhm(),
                                                'clct_log_sn': th_data_clct_mastr_log.clct_log_sn,
                                                'page_no': page_no
                                            })
                                            clct_sn_counter += 1

                                    # with session.begin() as conn:
                                    #     for row in extracted_data:
                                    #         columns = ', '.join(row.keys())
                                    #         # values = ', '.join([f"'{v}'" for v in row.values()])
                                    #         values = ', '.join([f"'{v}'" if v is not None else 'NULL' for v in row.values()])
                                    #         insert_stmt = f'''
                                    #             INSERT INTO {table_name} ({columns}) VALUES ({values});
                                    #         '''
                                    #         conn.execute(insert_stmt)
                                    # with session.begin() as conn:
                                    #     for row in extracted_data:
                                    #         columns = ', '.join(row.keys())
                                    #         values = ', '.join([f"'{v}'" if v is not None else 'NULL' for v in row.values()])
                                    #         insert_stmt = text(f'''
                                    #             INSERT INTO {table_name} ({columns}) VALUES ({values});
                                    #         ''')
                                    #         conn.execute(insert_stmt)
                                    # with session.begin() as conn:
                                    #     for row in extracted_data:
                                    #         columns = ', '.join(row.keys())
                                    #         # values = ', '.join([f"'{v}'" for v in row.values()])
                                    #         values = ', '.join([f"'{v}'" if v is not None else 'NULL' for v in row.values()])
                                    #         insert_stmt = f'''
                                    #             INSERT INTO {table_name} ({columns}) VALUES ({values});
                                    #         '''
                                    #         # ë¬¸ì œê°€ ë˜ëŠ” ì½œë¡  íŒ¨í„´ë§Œ ì „ê° ì½œë¡ ìœ¼ë¡œ êµì²´
                                    #         safe_stmt = insert_stmt.replace(':93ë™', 'ï¼š93ë™')
                                    #         conn.execute(text(safe_stmt))
                                    with session.begin() as conn:
                                        for row in extracted_data:
                                            columns = ', '.join(row.keys())
                                            
                                            # ì‘ì€ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                                            safe_values = []
                                            for v in row.values():
                                                if v is None:
                                                    safe_values.append('NULL')
                                                else:
                                                    # ì‘ì€ë”°ì˜´í‘œë¥¼ ë‘ ê°œë¡œ ì´ìŠ¤ì¼€ì´í”„ (ì²­ì£¼ì‹œì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°)
                                                    escaped_v = str(v).replace("'", "''")
                                                    safe_values.append(f"'{escaped_v}'")
                                            
                                            values = ', '.join(safe_values)
                                            insert_stmt = f'''
                                                INSERT INTO {table_name} ({columns}) VALUES ({values});
                                            '''
                                            # ì½œë¡  íŒ¨í„´ë„ í•¨ê»˜ ì²˜ë¦¬ (ë³´ì€êµ°ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°)
                                            safe_stmt = insert_stmt.replace(':93ë™', 'ï¼š93ë™')
                                            conn.execute(text(safe_stmt))
                                # í˜ì´ì§€ ì¦ê°€
                                # page_no += 1

                                # ì´ í˜ì´ì§€ ìˆ˜ == 1)
                                if total_page == 1:
                                    repeat_num += 1
                                    break
                                else:
                                    if page_no <= total_page:
                                        continue
                                    elif page_no > total_page:
                                        if params_len == 1:
                                            repeat_num += 1
                                            break
                                        elif params_len != 1:
                                            if repeat_num < params_len:
                                                page_no = 1
                                                repeat_num += 1
                                            else: 
                                                repeat_num += 1
                                                break
                            else:
                                logging.info(f"call_url_process resultmsg::: NO_DATA")
                                retry_num += 1
                                continue
                        else:
                            logging.info(f"call_url_process response_code::: {response_code}")
                            retry_num += 1
                            continue

                # íŒŒì¼ ì‚¬ì´ì¦ˆ í™•ì¸
                if os.path.exists(full_file_name):
                    file_size = os.path.getsize(full_file_name)
                logging.info(f"call_url file_name::: {file_name}, file_size::: {file_size}")
                logging.info(f"call_url::: ìˆ˜ì§‘ ë")

            except AirflowSkipException as e:
                raise e
            except Exception as e:
                logging.info(f"call_url Exception::: {e}")
                raise e
            
        # @task(trigger_rule='all_done')
        # def insert_data_info(collect_data_list,**kwargs):
        #     """
        #     DW ì ì¬ (tn_data_bsc_infoì— í•„ìš”í•œ ë°ì´í„°ë§Œ ê°ê° ì¶”ì¶œí•˜ì—¬ ì ì¬)
        #     params : collect_data_list, tdm_list_url_info, tdm_file_url_info, tdm_standard_url_info
        #     """
        #     tn_data_bsc_info = TnDataBscInfo(**collect_data_list['tn_data_bsc_info'])
        #     th_data_clct_mastr_log = ThDataClctMastrLog(**collect_data_list['th_data_clct_mastr_log'])
        #     data_crtr_pnttm = th_data_clct_mastr_log.data_crtr_pnttm
        #     dtst_cd = tn_data_bsc_info.dtst_cd

        #     try:
        #         with session.begin() as conn:
                    
        #             if dtst_cd in ['data919']:
        #                 # ëª©ë¡ê³¼ íŒŒì¼ ë°ì´í„° Join í›„ bscì— insertí•˜ëŠ” í•¨ìˆ˜
        #                 query = f"SELECT fn_data_file_data_list_updt('{data_crtr_pnttm}');"
        #                 conn.execute(query)
        #                 logging.info(f"Query executed: {query}")
        #                 logging.info(f"fn_data_file_data_list_updt completed successfully. Inserted {query} rows.")
                    
        #             if dtst_cd == 'data922':
        #                 # í‘œì¤€ ë°ì´í„° bscì— insertí•˜ëŠ” í•¨ìˆ˜
        #                 query = f"SELECT fn_data_std_data_list_updt('{data_crtr_pnttm}');"
        #                 conn.execute(query)
        #                 logging.info(f"Query executed: {query}")
        #                 logging.info(f"fn_data_std_data_list_updt completed successfully. Inserted {query} rows.")
            
        #     except Exception as e:
        #         logging.error(f"insert_data_info Exception for data_crtr_pnttm {data_crtr_pnttm}::: {e}")
        #         raise e


        # @task
        # def check_loading_result(collect_data_list):
        #     """
        #     DW ì ì¬ ê²°ê³¼ í™•ì¸
        #     params: collect_data_list
        #     """
        #     # tdm_list_url_info = TdmListUrlInfo(**collect_data_list['tdm_list_url_info'])
        #     # dw_tbl_phys_nm = TdmListUrlInfo.dw_tbl_phys_nm
        #     # tn_data_bsc_info_test = TnDataBscInfo(**collect_data_list['tn_data_bsc_info_test'])
        #     tn_data_bsc_info = TnDataBscInfo(**collect_data_list['tn_data_bsc_info'])
        #     dw_tbl_phys_nm = tn_data_bsc_info.dw_tbl_phys_nm
        #     th_data_clct_mastr_log = ThDataClctMastrLog(**collect_data_list['th_data_clct_mastr_log'])
        #     data_crtr_pnttm = th_data_clct_mastr_log.data_crtr_pnttm
            
        #     tn_clct_file_info = TnClctFileInfo(**collect_data_list['tn_clct_file_info'])
        #     log_full_file_path = collect_data_list['log_full_file_path']
        #     # dw_tbl_phys_nm = TnDataBscInfo.__tablename__

        #     result_count = 0
        #     get_count_stmt = f"""SELECT COUNT(data_crtr_pnttm) FROM {dw_tbl_phys_nm} WHERE data_crtr_pnttm = '{data_crtr_pnttm}'"""
        #     try:
        #         with session.begin() as conn:
        #             result_count = conn.execute(get_count_stmt).first()[0]
        #             th_data_clct_mastr_log = conn.get(ThDataClctMastrLog, collect_data_list['th_data_clct_mastr_log']['clct_log_sn'])
        #             th_data_clct_mastr_log.dw_rcrd_cnt = result_count
        #             CommonUtil.update_log_table(log_full_file_path, tn_clct_file_info, session, th_data_clct_mastr_log, CONST.STEP_DW_LDADNG, CONST.STTS_COMP, CONST.MSG_DW_LDADNG_COMP, "n")
        #             logging.info(f"check_loading_result dw_rcrd_cnt::: {result_count}")
        #     except Exception as e:
        #         logging.error(f"check_loading_result Exception::: {e}")
        #         raise e
        file_path = create_directory(collect_data_list)
        file_path >> call_url(collect_data_list, file_path) 
                
    collect_data_list = collect_data_info()
    call_url_process.expand(collect_data_list = collect_data_list)

dag_object = api_dw_month_data_1st()

# only run if the module is the main program
if __name__ == "__main__":
    conn_path = "../connections_minio_pg.yaml"
    dtst_cd = ""

    dag_object.test(
        execution_date=datetime(2025,7,28,9,00),
        conn_file_path=conn_path,
    )